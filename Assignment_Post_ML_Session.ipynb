{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## The Gradient Quest\n",
        "---\n",
        "##### Dataset link: https://www.kaggle.com/datasets/hellbuoy/car-price-prediction\n",
        "##### Objective: Understand Gradient Descent step-by-step through a coding quest."
      ],
      "metadata": {
        "id": "YKZnbquuaMNS"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h4xEpOZNaJFS"
      },
      "outputs": [],
      "source": [
        "# STAGE 1: The Gatekeeper\n",
        "\n",
        "# Task: Load the dataset \"CarPrice_Assignment.csv\"\n",
        "# Print the number of rows and columns.\n",
        "# Your output should print something like:\n",
        "# \"Shape: (205, 26)\"\n",
        "#\n",
        "# Hint: Use pandas. Once you get this right, your clue is the number of columns.\n",
        "\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "# TODO: Load the dataset into a dataframe called df and print its shape."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# STAGE 2: The Feature Forge\n",
        "\n",
        "# Task: Select 3 numerical features that you believe are most relevant to car price.\n",
        "# Example: \"enginesize\", \"horsepower\", \"curbweight\"\n",
        "# Create X and y arrays.\n",
        "#\n",
        "# Print the shape of X.\n",
        "# The clue for the next stage is (number of features * 2).\n",
        "#\n",
        "# Hint: Use df[['col1', 'col2', 'col3']] and df['price']\n",
        "\n",
        "# TODO: Select features and target, then print shape and clue."
      ],
      "metadata": {
        "id": "NQIAgu7UasAV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# STAGE 3: The Cost Chamber\n",
        "\n",
        "# Task: Implement the cost function for linear regression:\n",
        "# J(theta) = (1/2m) * Σ (hθ(xᵢ) - yᵢ)²\n",
        "#\n",
        "# Define a function `compute_cost(X, y, theta)` that returns the cost.\n",
        "# Use small dummy arrays to test correctness.\n",
        "# For example, if X = [[1], [2]], y = [[2], [4]], theta = [[1]],\n",
        "# the cost should be around 0.5 when predictions = [1,2].\n",
        "#\n",
        "# Print your computed cost.\n",
        "# Clue: Round the cost to nearest integer and print as \"CLUE 2: <value>\"\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "# TODO: Define compute_cost(X, y, theta) and print the clue."
      ],
      "metadata": {
        "id": "nUw4MErza1QR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# STAGE 4: The Gradient Gate\n",
        "\n",
        "# Task: Implement one step of Gradient Descent manually.\n",
        "# θ := θ - (α/m) * (Xᵀ · (Xθ - y))\n",
        "#\n",
        "# Write a function gradient_step(X, y, theta, alpha)\n",
        "# Run it once and print the updated theta.\n",
        "#\n",
        "# Clue: Print the sum of theta elements rounded to 2 decimals.\n",
        "# Example: CLUE 3: 1.54\n",
        "\n",
        "# TODO: Implement one step of GD and print clue."
      ],
      "metadata": {
        "id": "7edK2bila6fJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# STAGE 5: The Descent Spiral\n",
        "\n",
        "# Task: Implement full Gradient Descent loop for a few iterations.\n",
        "# Record the cost at each iteration.\n",
        "# Plot cost vs iteration to visualize convergence.\n",
        "#\n",
        "# Once done, print the cost at the last iteration (rounded to 2 decimals).\n",
        "# Clue: That cost is CLUE 4.\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# TODO: Implement gradient_descent(X, y, theta, alpha, iterations)\n",
        "# TODO: Plot cost history and print final cost as CLUE 4."
      ],
      "metadata": {
        "id": "StVOLboMa_T9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# STAGE 6: The Learning Rate Trial\n",
        "\n",
        "# Task: Try learning rates α = [0.001, 0.01, 0.1]\n",
        "# For each, run gradient descent for 500 iterations.\n",
        "# Plot the cost vs iteration for each α on the same plot.\n",
        "#\n",
        "# Your goal is to find which α converges fastest without diverging.\n",
        "# Print: \"CLUE 5: The best alpha is <value>\"\n",
        "\n",
        "# TODO: Run experiments for different alphas, plot and print clue."
      ],
      "metadata": {
        "id": "l7AwyQidbCi0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# STAGE 7: The Twin Peaks\n",
        "\n",
        "# Task: Train a LinearRegression model using sklearn on the same features.\n",
        "# Compare your theta (from gradient descent) with sklearn coefficients.\n",
        "# If they are numerically close, print \"CLUE 6: MATCH\"\n",
        "# Otherwise, print \"CLUE 6: TRY AGAIN\"\n",
        "\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# TODO: Train sklearn LinearRegression and compare coefficients.\n"
      ],
      "metadata": {
        "id": "2KwQNA35bHzy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# STAGE 8: Reflection\n",
        "\n",
        "# Task: Answer the following briefly in markdown cells (not code).\n",
        "# 1. What happens if the learning rate is too high?\n",
        "# 2. Why does feature scaling help gradient descent?\n",
        "# 3. How do your results compare to sklearn’s model?\n",
        "#\n",
        "# Final message: Print(\"THE QUEST IS COMPLETE\") once you’ve answered all questions!!!"
      ],
      "metadata": {
        "id": "zjoOKwwYbXrN"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}